# -*- coding: utf-8 -*-
"""ServiceNow_Assignment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12LPjREn8lIvYinLlFuUyM67EfZFga_FD

## **Assignment:**

1. Use a pre-trained google/flan-t5-small as the model.
2. Verify if the summariza'on task works.
3. Verify if the Q&A task works.
4. Verify if English to French transla'on task works.
5. Programma'cally print the names of all the model layers and their dimensions.
6. Programma'cally print the total number of parameters/weights in this model.
7. Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.
8. Verify if the Q&A task works aWer reseXng the weights of the above layer.
9. Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension
10. Reload the original google/flan-t5-small model.
11. Train the model for a Q&A task that takes a context as addi'onal input along with the
ques'on. You can use SQuAD dataset (h_ps://rajpurkar.github.io/SQuAD-explorer/ ) or the smaller Topioca dataset (h_ps://mcgill-nlp.github.io/topiocqa/) . Choose an appropriate task prefix/trigger word and jus'fy the choice.
12. Evaluate the quality of the model

## **1. Use a pre-trained google/flan-t5-small as the model.**
"""

# !pip install transformers
# !pip install evaluate
# !pip install datasets
# !pip install SentencePiece

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

"""## **2.Verify if the summarization task works.**"""

def summarize():
  input_text = """Hear the term artificial intelligence (AI) and you might think of self-driving cars, robots, ChatGPT or other AI chatbots, and artificially created images. But it's also important to look behind the outputs of AI and understand how the technology works and its impacts for this and future generations.
  AI is a concept that has been around, formally, since the 1950s, when it was defined as a machine's ability to perform a task that would've previously required human intelligence. This is quite a broad definition and one that has been modified over decades of research and technological advancements.
  When you consider assigning intelligence to a machine, such as a computer, it makes sense to start by defining the term 'intelligence' -- especially when you want to determine if an artificial system is truly deserving of it.
  """
  input_ids = tokenizer(input_text, max_length=2048, truncation=True, return_tensors="pt").input_ids
  output_ids = model.generate(input_ids, max_length=50)
  output_text = tokenizer.decode(output_ids[0],
                                skip_special_tokens=True)

  print(f"Summarized Text: {output_text}")
  return

summarize()

"""## **3.Verify if the Q&A task works.**"""

def generate_answer(model, tokenizer):

  context = """In the visual arts, the Normans did not have the rich and distinctive traditions of the
              cultures they conquered. However, in the early 11th century the dukes began a programme of church reform,
              encouraging the Cluniac reform of monasteries and patronising intellectual pursuits, especially the
              proliferation of scriptoria and the reconstitution of a compilation of lost illuminated manuscripts.
              The church was utilised by the dukes as a unifying force for their disparate duchy. The chief monasteries
              taking part in this "renaissance" of Norman art and scholarship were Mont-Saint-Michel, Fécamp, Jumièges,
              Bec, Saint-Ouen, Saint-Evroul, and Saint-Wandrille. These centres were in contact with the so-called
              "Winchester school", which channeled a pure Carolingian artistic tradition to Normandy. In the final
              decade of the 11th and first of the 12th century, Normandy experienced a golden age of illustrated
              manuscripts, but it was brief and the major scriptoria of Normandy ceased to function after the midpoint
              of the century."""

  question = 'When did the church reform begin?'

  # Encode the question and context into input IDs.
  input_ids = tokenizer(question, context, return_tensors="pt").input_ids

  # Generate the answer.
  output_ids = model.generate(input_ids, max_length=50)
  output_text = tokenizer.decode(output_ids[0],
                                 skip_special_tokens=True)

  print(f"Answer: {output_text}")
  return


generate_answer(model, tokenizer)

"""## **4.Verify if English to French translation task works.**"""

def translation():
  english_sentence = "Translate from English to French: Eiffel Tower is very beautiful."
  input_ids = tokenizer(english_sentence, return_tensors="pt").input_ids

  output_ids = model.generate(input_ids, max_length=50)
  output_text = tokenizer.decode(output_ids[0],
                                skip_special_tokens=True)

  print(output_text)
  return

translation()

"""## **5.Programma'cally print the names of all the model layers and their dimensions.**"""

# Names and dimensions of all the model layers
for name, param in model.named_parameters():
    print(name, param.size())

"""## **6.Programma'cally print the total number of parameters/weights in this model.**"""

# Total number of parameters/weights in the model
total_params = sum(p.numel() for p in model.parameters())
print(f"Total number of parameters/weights: {total_params}")

"""### **7.Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros.**"""

# tensor in the final layer of the decoder
decoder_final_layer_norm_weight = model.decoder.final_layer_norm.weight
decoder_final_layer_norm_weight.data.zero_() # Settinng to zero
print("Decoder final layer norm weight after setting to zero:", decoder_final_layer_norm_weight)

"""## **8.Verify if the Q&A task works after resetting the weights of the above layer.**"""

def generate_answer(model, tokenizer, question, context):

  input_ids = tokenizer(question, context, return_tensors="pt").input_ids
  output_ids = model.generate(input_ids, max_length=50)
  output_text = tokenizer.decode(output_ids[0],
                                 skip_special_tokens=True)
  return output_text

# Considering a sample context from the above.
context = """In the visual arts, the Normans did not have the rich and distinctive traditions of the
cultures they conquered. However, in the early 11th century the dukes began a programme of church reform,
encouraging the Cluniac reform of monasteries and patronising intellectual pursuits, especially the
proliferation of scriptoria and the reconstitution of a compilation of lost illuminated manuscripts.
The church was utilised by the dukes as a unifying force for their disparate duchy. The chief monasteries
taking part in this "renaissance" of Norman art and scholarship were Mont-Saint-Michel, Fécamp, Jumièges,
Bec, Saint-Ouen, Saint-Evroul, and Saint-Wandrille. These centres were in contact with the so-called
"Winchester school", which channeled a pure Carolingian artistic tradition to Normandy. In the final
decade of the 11th and first of the 12th century, Normandy experienced a golden age of illustrated
manuscripts, but it was brief and the major scriptoria of Normandy ceased to function after the midpoint
of the century."""

question = 'When did the church reform begin?'

generated_answer = generate_answer(model, tokenizer, question, context)
print(generated_answer)

"""## *Nothing is given as output after resetting the final layer with zeroes*

## **9.Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension**
"""

decoder_final_layer_norm_weight = model.decoder.final_layer_norm.weight
new_decoder_final_layer_norm_weight = torch.nn.Linear(decoder_final_layer_norm_weight.size()[0], 128)
new_decoder_final_layer_norm_weight.weight.data.zero_()
new_decoder_final_layer_norm_weight.bias.data.zero_()
model.decoder.final_layer_norm.weight = torch.nn.Parameter(new_decoder_final_layer_norm_weight.weight.data)
for layer in model.decoder.layers:
    layer.output.dense.weight.data = layer.output.dense.weight.data[:, :128]
    layer.output.dense.bias.data = layer.output.dense.bias.data[:128]

"""## **10. Reload the original google/flan-t5-small model.**"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("google/flan-t5-small")
model = AutoModelForSeq2SeqLM.from_pretrained("google/flan-t5-small")

"""## **11. Train the model for a Q&A task that takes a context as addi'onal input along with the ques'on. You can use SQuAD dataset (h_ps://rajpurkar.github.io/SQuAD-explorer/ ) or the smaller Topioca dataset (h_ps://mcgill-nlp.github.io/topiocqa/) . Choose an appropriate task prefix/trigger word and jus'fy the choice.**"""

import numpy as np
from datasets import load_dataset
from tqdm.auto import tqdm
import torch
from torch import nn
from torch.utils.data import DataLoader
import evaluate
from transformers import (
    T5Tokenizer,
    T5ForConditionalGeneration,
    DataCollatorForSeq2Seq,
    default_data_collator,
    AdamW,
    get_scheduler
)

def set_seed(seed):
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

def preprocess_function(examples, padding="max_length", max_input_length=512, max_target_length=32):

    answers = [example['text'][0] for example in examples["answers"]]
    questions = [q.strip() for q in examples["question"]]
    contexts = [context.strip() for context in examples["context"]]

    prompts = [f'Given the following passage:\n{context}\nAnswer the question:\n{question}\n'
               for context, question in list(zip(contexts, questions))]

    inputs = tokenizer(
        prompts,
        max_length=max_input_length,
        truncation=True,
        padding=True,
    )
    labels = tokenizer(answers, max_length=max_target_length, padding=padding, truncation=True)
    inputs["labels"] = labels["input_ids"]
    return inputs

def compute_metrics(preds, labels, valiation_set):
    final_preds = [[token for token in pred_list if token != tokenizer.eos_token_id] for pred_list in preds]
    decoded_preds = tokenizer.batch_decode(final_preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    decoded_preds = [pred.strip() for pred in decoded_preds]
    decoded_labels = [label.strip() for label in decoded_labels]

    references = []
    final_preds = []

    for i, pred in enumerate(decoded_preds):
        final_preds.append({
            "id": validation_set[i]['id'],
            "prediction_text": pred
        })
        references.append({
            "id": validation_set[i]['id'],
            "answers": validation_set[i]['answers']
        })
    result = metric.compute(predictions=final_preds, references=references)
    print(result)
    return result


def create_data_loader():
  train_dataloader = DataLoader(
      tokenized_dataset["train"], shuffle=True, batch_size=8, collate_fn=data_collator
  )
  eval_dataloader = DataLoader(
      tokenized_dataset["validation"], batch_size=8, collate_fn=data_collator
  )
  return train_dataloader, eval_dataloader



def evaluate_model():
    model.eval()
    predictions = []
    labels = []

    with torch.no_grad():
        for batch in tqdm(eval_dataloader):
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)
            logits = outputs.logits
            val_loss = outputs.loss
            preds = torch.argmax(logits, dim=-1)
            predictions.extend(preds.tolist())
            labels.extend(batch['labels'].tolist())

    print(f"Epoch [{num_epochs}/{num_training_steps}], Val_loss: {val_loss.item():.4f}")
    compute_metrics(predictions, labels, validation_set)
    model.train()



# LOOP

set_seed(42)

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")
model.to(device)

dataset = load_dataset("squad")
metric = evaluate.load("squad")
tokenized_dataset = dataset.map(preprocess_function, batched=True)

data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    pad_to_multiple_of=8
)

validation_set = [example for example in tokenized_dataset["validation"]]

tokenized_dataset['train'] = tokenized_dataset['train'].remove_columns(["title","id", "context", "question", "answers"])
tokenized_dataset["validation"] = tokenized_dataset["validation"].remove_columns(["title", "id", "context", "question", "answers"])

train_dataloader, eval_dataloader = create_data_loader()

optimizer = AdamW(model.parameters(), lr=5e-5)

num_epochs = 1
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)
# Training
progress_bar = tqdm(range(num_training_steps))
step = 0

for epoch in range(num_epochs):
    model.train()

    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()

        if step % 500 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{num_training_steps}], lr: {optimizer.param_groups[0]['lr']}, Loss: {loss.item()}")

        if step > 0 and step % 1000 == 0:
            evaluate_model()

        step += 1

    model.save_pretrained(f"{checkpoint_dir}/checkpoint_{epoch}")

"""## **12. Evaluate the model**"""

def evaluate_squad(model):
    dataset = load_dataset("squad", split='validation')
    sample = dataset[int(np.random.randint(len(dataset), size=1)[0])]

    context, question, actual_answer = sample['context'],  sample['question'], sample["answers"]
    input_text = f'Given the following:{context}. Answer the following:{question}'
    input_ids = tokenizer(input_text,
                          max_length=512,
                          truncation=True,
                          return_tensors="pt"
                          ).input_ids.to(device)
    outputs = model.generate(input_ids)
    result = tokenizer.batch_decode(outputs,
                                    skip_special_tokens=True)
    print("Predicted Answer:", result)

evaluate_squad(model)